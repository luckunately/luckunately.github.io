\documentclass[a4 paper, 10pt]{article}
\usepackage{graphicx,xcolor} % Required for inserting images

\definecolor{mylinks}{RGB}{21, 92, 78} % you can change the color from here

\usepackage[T1]{fontenc}
\usepackage{tgbonum}
\usepackage[top=2cm,bottom=2cm,left=1.5cm,right=1.5cm]{geometry}
\usepackage{fontawesome5}
\usepackage{tabularx,tabulary,multirow}

\usepackage[colorlinks=true,urlcolor=mylinks,linkcolor=mylinks
]{hyperref}
\usepackage[skip=10pt plus1pt, indent=40pt]{parskip}
\usepackage{blindtext}
\usepackage{hyperref}
\usepackage{indentfirst}
% \usepackage{etoolbox}

% I used this guy's template
% \title{SoP_Template}
% \author{Mayukh Chakrabarty}
% \date{29 September 2023}



\begin{document}
\newcommand{\ifstringequal}[4]{%
	\ifnum\pdfstrcmp{#1}{#2}=0
	#3%
	\else
	#4%
	\fi
}


\newcommand{\theSchoolFullName}{the University of Toronto}
\newcommand{\theFacultyMember}{Prof. Mark Jeffrey, Prof. Natalie Jerger and Prof. Vaughn Betz}
% \newcommand{\theSchoolFullName}{McGill University}
% \newcommand{\theFacultyMember}{Prof. Christophe Dubach}
% \newcommand{\theSchoolFullName}{University of British Columbia}
\newcommand{\theDepartment}{the department of Electrical and Computer Engineering}
% \newcommand{\theDepartment}{Computer Science}

\pagestyle{empty} % suppresses page numbers

%header
{\fontfamily{qbk}\selectfont % font can be changed from https://www.overleaf.com/learn/latex/Font_typefaces

	\begin{center}
		\begin{minipage}{.9\textwidth}
			\Large{\textbf{STATEMENT OF PURPOSE}}
		\end{minipage}

		\begin{tabularx}{.8\textwidth}{X r X}
			Xingyu Wang (Tom)                                                               \\
			BASC in Computer Engineering            \\
			University of British Columbia 
		\end{tabularx}
		\par\noindent\rule{\textwidth}{1.25pt}
	\end{center}

}

\fontfamily{ptm}\selectfont % font can be changed from https://www.overleaf.com/learn/latex/Font_typefaces

This Statement of Purpose is for the application to the graduate program in \theDepartment{} at \theSchoolFullName{}.

\section*{Interests of Study}
So far, the charm of studying in computers in my opinion is how I can make computations {faster and cheaper}, whether it is through \textbf{software optimization} or \textbf{hardware resources}. Nowadays, as the demand for computations grows since the evolution of {AI}, the need for {faster and cheaper computations} is more urgent than ever.

My primary interest lies in \textbf{computer architecture}, with a particular focus on \textbf{accelerating ML workloads}. As models become more complex—moving from simple classifiers to deep neural networks and transformers—the computational requirements grow rapidly, demanding efficient processing of large-scale data and high-dimensional tensors.

Through my research experiences, I have come to appreciate the intricate relationship between ML model training/inference and the underlying hardware. Training deep neural networks involves massive parallel computations, frequent memory accesses, and complex data movement between different memory hierarchies. The efficiency of these operations is tightly coupled with hardware characteristics such as memory bandwidth, cache organization, and the availability of specialized compute units (e.g., GPUs, TPUs, or FPGAs).

For example, as a research assistant under Prof. Alexandra Fedorova, I worked on ML-based cache and page prefetching, where I used various models (LSTM, MLP, transformer, etc.) to predict memory access patterns and reduce latency. This project required analyzing memory traces, experimenting with various neural network architectures, and tuning hyperparameters to improve prediction accuracy. It gave me valuable insights into the challenges of applying ML to system-level problems and the importance of tailoring solutions to specific workloads.

In another project, I focused on optimizing GPU memory usage for large language models (vLLM). I explored how GPU memory access patterns differ from traditional OS-level management and applied techniques such as memory prefetching and scheduling to reduce latency during model inference. By experimenting with different scheduling algorithms, I was able to improve memory allocation efficiency and overall system performance. These experiences deepened my understanding of memory management and scheduling in the context of ML workloads and sparked my interest in further exploring this area.

From these projects, I saw that hardware limits often become the bottleneck for ML workloads. Improving memory access, reducing latency, and designing efficient data paths can make a real difference in performance. I want to work on practical solutions—like better memory scheduling or custom accelerators—that directly improve how ML models run. My goal is to build systems where hardware and software work together for faster, more efficient ML.

Inspired by Prof. Andrew Boutros's PhD thesis, I have identified several specific areas I would like to explore during my graduate studies:
\begin{enumerate}
    \item I am interested in investigating the potential of \textbf{Customized accelerators for specific ML tasks} (Let's break the GPU Monopoly!). This involves understanding the design and implementation of {custom hardware solutions} to speed up inference or training processes for specific workloads. The prototype will start with {FPGAs}. FPGAs are powerful if we know how to effectively program and utilize their parallel processing capabilities and adaptability. Depending on the results, I would like to explore the possibility of transitioning to \textbf{ASIC design} or move on to \textbf{board-level reconfigurable computing}.
    \item With the experience I have in {memory management}, I want to explore how to optimize {memory access patterns} and {data locality} for ML workloads on software-programmable customized accelerators. This includes techniques like customized-compiler, scheduling, interconnects, off-chip communication, and memory hierarchy design that is specifically tailored for specific ML models or tasks.
    \item At the same time, I also want to explore flattening the model directly onto the {hardware circuit}, bypassing the traditional software stack. This could involve using \textbf{Domain Specific High-Level Synthesis (HLS)} tools to convert high-level descriptions of ML models directly into hardware implementations. (Accelerators which require complex software stacks like TPUs seem like another form of ... GPU to me.) After finding a few promising approaches on different workloads, I want to implement and seek the possibility to \textbf{generalize} these approaches for broader applications. (For each specific feature of the workload, pack up a block of {IP} made for it, and then combine these blocks to form a more complex accelerator for a broader range of workloads. Modular design?)
\end{enumerate}

While I am excited about hardware acceleration—such as designing customized accelerators and exploring FPGA-based solutions—I am equally fascinated by \textbf{software-level optimizations},  \textbf{CUDA acceleration} and \textbf{parallel computing}. Leveraging CUDA for GPU programming has shown me how efficient parallel algorithms and memory management can dramatically improve ML inference and training speed. I am eager to further explore how \textbf{hardware and software co-design}. Despite lack of experience in this area, I started learning CUDA and parallel computing on my own time after work, and I am excited to deepen my knowledge in this area during my graduate studies.

\section*{Why Graduate Study?}
I have been working as an {FPGA Soft IP Engineer} in Altera for a few months now, and I have realized that the work I do in the industry is quite different from what I expect. I want to be able to develop {new tools and technologies} that can make a difference in the industry and society. I want to be able to work on {innovative projects} that can push the boundaries of what is possible.

I believe that graduate study will provide me with the opportunity to {deepen my understanding} of the field and to develop the skills necessary to conduct {research} and contribute to the {advancement of knowledge}. I want to be able to work on {cutting-edge research projects} and to collaborate with {experts in the field}. I believe that this will help me to achieve my long-term goal of contributing to the {advancement of technology}.

The faculty members such as \theFacultyMember{} at \theSchoolFullName{} are leading experts in their respective fields, and I am excited about the opportunity to learn from them and to work with them on research projects. 
% I have read several papers published by the faculty members that I selected as my potential advisors, and I am impressed by their work. I have sent emails to a few of them listing my thoughts on their papers and expressing my interest in working with them. The email address is {fortily@student.ubc.ca} if you missed it. I am also impressed by the resources and facilities available at \theSchoolFullName{}, which I believe will provide me with the tools necessary to succeed in my studies.

% \section*{Why Me?}
% I believe I am a strong candidate for the graduate program in \theDepartment{} at \theSchoolFullName{} for several reasons:
% \begin{itemize}
%     \item \textbf{Strong Academic Background:} I have a solid foundation in {computer engineering}, with a focus on both {hardware and software} aspects. My coursework has provided me with a comprehensive understanding of the field (\textbf{Computer Architecture}, \textbf{Digital Logic Design}, \textbf{Operating Systems}, \textbf{ML}), and I have consistently performed well academically.
%     \item \textbf{Research Experience:} I have gained substantial research experience through two significant projects. \begin{itemize}
% 		\label{prefetching}
% 		\item As a research assistant under Prof. Alexandra Fedorova, I worked on \textbf{ML-based cache and page prefetching}, where I tried to predict the memory access patterns of applications to reduce latency. I started with training an LSTM model on collected memory traces of specific programs, to predict future memory accesses. While the LSTM model showed potential in relatively simple scenarios, it struggled with more complex patterns. To address this, I experimented with various architectures (Attention layer, MLP and transformer) and hyperparameters, ultimately improving the model's performance. This project provided me with valuable insights into the challenges of applying ML to system-level problems and how to customize models for specific tasks.
% 		\label{vllm}
% 		\item In another project focused on \textbf{vLLM memory management}, I explored optimizing GPU memory usage for large language models, discovering the unique characteristics of GPU memory access patterns compared to traditional OS-level management. This project is motivated by the nature of vLLM, where vLLM manages the memory of GPUs in blocks as virtual memory, which encouraged me to apply OS techniques learned from \hyperref[prefetching]{my previous project} to optimize memory allocation and scheduling. The initial tackle was to prefetch memory pages before they are needed, reducing latency during model inference. However, different from traditional OS, ML workloads do not have the same level of randomness in memory access patterns, making it unnecessary to predict which pages to prefetch. While working on this project, I found that the scheduling of memory requests led to some additional memory swap-in-swap-out overhead, which is not ideal for latency-sensitive applications. To address this, I explored various scheduling algorithms to optimize the order of memory requests, ultimately improving the overall performance of the system. The report can be found on \hyperlink{https://luckunately.github.io/courses/CPEN511/Final_report.pdf}{my GitHub} This project has deepened my understanding of memory management and scheduling in the context of ML workloads and has sparked my interest in further exploring this area.
% 		\item These experiences have not only enhanced my \textbf{technical skills} but also taught me how to approach \textbf{complex problems} and work collaboratively in a \textbf{research setting}.
% 	\end{itemize}
% 	\label{Altera}
%     \item \textbf{Relevant Work Experience:} My current role as an {FPGA Soft IP Engineer} has given me practical experience in \textbf{hardware design} and \textbf{software stack for custom hardware}. I worked on large-scale traffic generator IP for testing efficiency of memory operations on the on-chip memory such as HBM and DDRRAM. I have gained knowledge with NoC interfaces, AXI protocol, and FPGA toolchains. This industry experience complements my {academic background} and {research skills}, making me well-prepared for the challenges of graduate study.
%     \item \textbf{Passion for Learning and Innovation:} I am deeply passionate about the field of {computer engineering} and am eager to explore {new ideas and technologies}. I am motivated to contribute to the \textbf{advancement of knowledge} and to make a meaningful impact in the field.
%     \item \textbf{Clear Research Interests:} I have a clear vision of my research interests, particularly in the areas of \textbf{computer architecture} and \textbf{ML acceleration}. This focus will allow me to make significant contributions to the field during my graduate studies.
% \end{itemize}

\end{document}