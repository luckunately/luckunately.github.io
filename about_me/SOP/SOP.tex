\documentclass[a4 paper, 10pt]{article}
\usepackage{graphicx,xcolor} % Required for inserting images

\definecolor{mylinks}{RGB}{21, 92, 78} % you can change the color from here

\usepackage[T1]{fontenc}
\usepackage{tgbonum}
\usepackage[top=2cm,bottom=2cm,left=1.5cm,right=1.5cm]{geometry}
\usepackage{fontawesome5}
\usepackage{tabularx,tabulary,multirow}

\usepackage[colorlinks=true,urlcolor=mylinks,linkcolor=mylinks
]{hyperref}
\usepackage[skip=10pt plus1pt, indent=40pt]{parskip}
\usepackage{blindtext}
\usepackage{hyperref}
% \usepackage{etoolbox}

% I used this guy's template
% \title{SoP_Template}
% \author{Mayukh Chakrabarty}
% \date{29 September 2023}



\begin{document}
\newcommand{\ifstringequal}[4]{%
  \ifnum\pdfstrcmp{#1}{#2}=0
  #3%
  \else
  #4%
  \fi
}


\newcommand{\schoolName}{UBC}
\newcommand{\theSchoolFullName}{University of British Columbia}
\newcommand{\theDepartment}{Electrical and Computer Engineering}
\csdef{path-UBC}{bank/ubc.tex}
\newcommand{\ccc}{bank/ubc.tex}




% \ifstrequal{\schoolName}{UBC}{
    
%     \ifstrequal{\schoolName}{U of T}{
%         \input{uoft_file.tex}
%     }{
%         % Default case or other cases
%         % \input{default_file.tex}
%     }
% }

\pagestyle{empty} % suppresses page numbers

%header
{\fontfamily{qbk}\selectfont % font can be changed from https://www.overleaf.com/learn/latex/Font_typefaces

\begin{center}
    \begin{minipage}{.9\textwidth}
        \Large{\textbf{STATEMENT OF PURPOSE}}    
    \end{minipage}

    \begin{tabularx}{.8\textwidth}{X r X}
    Xingyu Wang (Tom)                 & \multicolumn{2}{r}{\faIcon{mobile} +1 604-388-5164}     \\
        BASC in Computer Engineering  & \multicolumn{2}{r}{\faIcon{envelope}  \href{tomxingyuwang@gmail.com}{tomxingyuwang@gmail.com}} \\
        University of British Columbia  &  \multicolumn{2}{r}{\faIcon{linkedin} \href{www.linkedin.com/in/tom-wang-554904220/}{linkedin-profile}}
    \end{tabularx}
    \par\noindent\rule{\textwidth}{1.25pt}
\end{center}

}

{\fontfamily{ptm}\selectfont % font can be changed from https://www.overleaf.com/learn/latex/Font_typefaces

This Staement of Purpose is for the application to the graduate program in department of \theDepartment{} at \theSchoolFullName{}.

\section*{Interests of study}
So far, the charm of \theDepartment{} in my opinion is how I can make computations faster and cheaper, whether it is through software optimization or hardware resources. Nowadays as the demand of computations grows since the evolution of AI, the need of faster and cheaper computations is more urgent than ever.

I am particularly interested in the field of computer architecture especially in accelerating ML workloads. So far, I have some experience with memory management in \hyperref[prefetching]{CPUs} and \hyperref[vllm]{GPUs} which still attracts me, but also, I want to explore in interconnects, FPGA-based specialized hardware accelerators, etc.

Currently I have two plans in my mind:
\begin{itemize}
    \item \textbf{Plan A:} I am interested in investigating the potential of FPGA-based accelerators for ML tasks (Let's break GPU Monopoly!). This involves understanding the design and implementation of custom hardware solutions to speed up inference or training processes for specific workloads. I think this is possible with FPGA if we know how to effectively program and utilize their parallel processing capabilities. 
    After finding a few promising approaches on different workloads, I want to implement and seek the possibility to generalize these approaches for broader applications. (For each specific feature of the workload, pack up a block of FPGA IP made for it, and then combine these blocks to form a more complex accelerator for a broader range of workloads. HLS or FPGA IP programming in a sense?)
    \item \textbf{Plan B:} I want to dive deeper into memory management and scheduling techniques, especially in the context of ML workloads. This includes exploring advanced memory strategies for CPUs, GPUs or even NPUs. Nowadays, SOC gets so complex that efficient memory management is crucial for performance. I want to investigate how to optimize memory access patterns and improve data locality to enhance the overall efficiency of ML workloads.
\end{itemize}
%     \item \textbf{And the list goes on...} There are areas that I have not explored yet like in memory processing, quantum computing, etc. I would like to explore these areas as well.
% \end{itemize}
% \section*{Future Goals}
% Right now I am more leaning towards the academia, to be added...
\section*{Why \theSchoolFullName{}?}

\ifdefstring{\schoolName}{U of T}{\input{bank/ut.tex}}{}
\ifdefstring{\schoolName}{UBC}{\input{bank/ubc.tex}}{}

\section*{Motivation}
When I first entered the university, I was not exposed of any details of how computations are done,
\section*{My Background}

\subsection*{Academic Background}
I took various courses in computer engineering, electrical engineering and computer science at UBC. 

\begin{itemize}
    \item For hardware, I took courses like Digital Logic Design, Computer Architecture, accelerator design and more. 
    \item For software, I took courses like Operating Systems, Machine Learning and more.
\end{itemize}

All the above courses have given me a solid foundation in both hardware and software, which I believe is essential for the graduate study in \theDepartment{} at \theSchoolFullName{}.
% \subsection*{Awards and Scholarships}
% \begin{enumerate}
%     \item I have received the Dean's Honour List for 3 years in a row, which is the recognition of my academic achievement at UBC.
%     \item I also received the NSERC Undergraduate Student Research Award at the third year summer, which encouraged me to pursue research in the field of computer engineering. It is a funding that provides 4-month minimum wage for undergraduate students to work on research projects.
% \end{enumerate}

\subsection*{Work Experience}

\subsubsection*{ML Prefetching for Page Faults: May 2024 -- April 2025} 
\label{prefetching}
I worked as a research assistant under Prof. Alexandra Fedorova after receiving an NSERC award, focusing on ML-based cache and page prefetching. My main task was to collect memory traces and adapt an LSTM model for page fault prediction, implemented in PyTorch and trained on a UBC GPU server. Results showed cache models perform poorly on page faults, highlighting distinct access patterns. We began exploring transformer-based models, but I paused the project for a full-time internship. 

\subsubsection*{vLLM Memory Management: Jan 2024 -- April 2025}
\label{vllm}
I also worked on memory management for vLLM, a framework for efficient large language model inference in my \textbf{Advanced Computer Architecture} course. Initially, my goal was to optimize GPU memory usage with the techniques I learned in \hyperref[prefetching]{ML Prefetching for Page Faults} since the idea of vLLM is to manage GPU memory with virtual-memory-like techniques. However, I found that for GPU workloads, the memory access patterns are close to deterministic which is quite different from OS-level memory management. 

\subsubsection*{FPGA Soft IP Engineer: May 2025 -- Present}
\label{fpga}
As an FPGA Soft IP Engineer in \textbf{Altera}, I am currently working on developing soft IP cores for FPGA platforms, more specifically focusing on the subsystems of HBM (High Bandwidth Memory). This role involves designing and optimizing hardware components for specific applications, leveraging my background in computer architecture and digital design. I am gaining hands-on experience with hardware description languages and FPGA development tools, further bridging the gap between software and hardware design.
\end{document}
